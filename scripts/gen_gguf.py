import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel
import os
import subprocess
import sys

# --- 1. è·¯å¾„é…ç½® (æ‚¨çš„è·¯å¾„éƒ½æ˜¯æ­£ç¡®çš„) ---

BASE_MODEL_ID = "hfl/chinese-alpaca-2-1.3b"
LORA_ADAPTER_PATH = "/root/robocon-llm/outputs/chinese-alpaca-2-1.3b-lora-finetuned-robocon"
MERGED_MODEL_DIR = "/root/robocon-llm/merged-model-alpaca-final"
LLAMA_CPP_DIR = "/root/llama.cpp"

# --- æ–°çš„ä¸¤æ­¥GGUFè½¬æ¢é…ç½® ---
INTERMEDIATE_GGUF_TYPE = "f16"  # æ­¥éª¤2: è½¬æ¢æ—¶çš„ä¸­é—´æ ¼å¼
FINAL_GGUF_TYPE = "q4_k_m"   # æ­¥éª¤3: æœ€ç»ˆæƒ³è¦çš„é‡åŒ–æ ¼å¼

GGUF_NAME_INTERMEDIATE = f"r2-robot.{INTERMEDIATE_GGUF_TYPE}.gguf"
GGUF_NAME_FINAL = f"r2-robot.{FINAL_GGUF_TYPE}.gguf"

# -----------------------------------------------

def main():
    
    # --- æ­¥éª¤ 1: åˆå¹¶ LoRA æƒé‡ (æˆ–è·³è¿‡) ---
    print(f"--- æ­¥éª¤ 1: æ£€æŸ¥æƒé‡åˆå¹¶ ---")
    
    final_gguf_path = os.path.join(MERGED_MODEL_DIR, GGUF_NAME_FINAL)
    intermediate_gguf_path = os.path.join(MERGED_MODEL_DIR, GGUF_NAME_INTERMEDIATE)

    if os.path.exists(final_gguf_path):
        print(f"ğŸ‰ æœ€ç»ˆ GGUF æ–‡ä»¶ {final_gguf_path} å·²å­˜åœ¨.")
        print("æ‰€æœ‰æ­¥éª¤å·²å®Œæˆ, é€€å‡ºè„šæœ¬.")
        return

    if not os.path.exists(MERGED_MODEL_DIR):
        print(f"åŠ è½½åŸºç¡€æ¨¡å‹: {BASE_MODEL_ID}")
        base_model = AutoModelForCausalLM.from_pretrained(
            BASE_MODEL_ID,
            torch_dtype=torch.float16,
            device_map="auto"
        )
        tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_ID)
        print(f"åŠ è½½LoRAé€‚é…å™¨: {LORA_ADAPTER_PATH}")
        model = PeftModel.from_pretrained(base_model, LORA_ADAPTER_PATH)
        print("æ­£åœ¨åˆå¹¶æƒé‡...")
        model = model.merge_and_unload()
        
        print("æ­£åœ¨ä¿®æ­£åŸºç¡€æ¨¡å‹çš„ generation_config.json...")
        if hasattr(model, "generation_config") and model.generation_config.do_sample is False:
            model.generation_config.temperature = None
            model.generation_config.top_p = None
        print("Config ä¿®æ­£å®Œæ¯•.")

        print(f"ä¿å­˜åˆå¹¶åçš„å®Œæ•´æ¨¡å‹åˆ°: {MERGED_MODEL_DIR}")
        os.makedirs(MERGED_MODEL_DIR, exist_ok=True)
        model.save_pretrained(MERGED_MODEL_DIR)
        tokenizer.save_pretrained(MERGED_MODEL_DIR)
        print(f"--- æ­¥éª¤ 1: æƒé‡åˆå¹¶å®Œæˆ ---")
    else:
        print(f"æ£€æµ‹åˆ°å·²åˆå¹¶çš„æ¨¡å‹ç›®å½•: {MERGED_MODEL_DIR}, è·³è¿‡åˆå¹¶.")


    # --- æ­¥éª¤ 2: è½¬æ¢ä¸º f16 GGUF ---
    print(f"\n--- æ­¥éª¤ 2: è½¬æ¢ä¸º {INTERMEDIATE_GGUF_TYPE} GGUF ---")

    convert_script = os.path.join(LLAMA_CPP_DIR, "convert_hf_to_gguf.py")
    
    if not os.path.exists(intermediate_gguf_path):
        command = [
            sys.executable,
            convert_script,
            MERGED_MODEL_DIR,
            "--outfile",
            intermediate_gguf_path,
            "--outtype",
            INTERMEDIATE_GGUF_TYPE
        ]

        print(f"æ‰§è¡Œå‘½ä»¤: {' '.join(command)}")
        try:
            subprocess.run(command, check=True)
            print(f"--- æ­¥éª¤ 2: è½¬æ¢ä¸º {INTERMEDIATE_GGUF_TYPE} GGUF æˆåŠŸ! ---")
        except subprocess.CalledProcessError as e:
            print(f"GGUF è½¬æ¢å¤±è´¥. é”™è¯¯ä¿¡æ¯: {e}")
            return
    else:
        print(f"æ£€æµ‹åˆ°å·²å­˜åœ¨çš„ {INTERMEDIATE_GGUF_TYPE} GGUF æ–‡ä»¶, è·³è¿‡è½¬æ¢.")
        

    # --- æ­¥éª¤ 3: é‡åŒ–ä¸º q4_k_m ---
    print(f"\n--- æ­¥éª¤ 3: é‡åŒ–ä¸º {FINAL_GGUF_TYPE} ---")

    #
    # ===================================================================
    # !! å…³é”®ä¿®æ­£(v5) !!
    # ä¿®æ­£äº† 'quantize' ä¸º 'llama-quantize'
    # ===================================================================
    quantize_exe = os.path.join(LLAMA_CPP_DIR, "build/bin/llama-quantize")
    
    if not os.path.exists(quantize_exe):
        print(f"é”™è¯¯: æ— æ³•åœ¨ {quantize_exe} æ‰¾åˆ° 'llama-quantize' å¯æ‰§è¡Œæ–‡ä»¶.")
        print(f"è¯·ç¡®ä¿æ‚¨å·²ç»åœ¨ {LLAMA_CPP_DIR}/build ç›®å½•ä¸­æˆåŠŸè¿è¡Œäº† 'cmake --build .' å‘½ä»¤!")
        return
        
    command = [
        quantize_exe,
        intermediate_gguf_path,  # è¾“å…¥æ–‡ä»¶
        final_gguf_path,         # è¾“å‡ºæ–‡ä»¶
        FINAL_GGUF_TYPE          # ç›®æ ‡ç±»å‹
    ]

    print(f"æ‰§è¡Œå‘½ä»¤: {' '.join(command)}")
    try:
        subprocess.run(command, check=True)
        print(f"--- æ­¥éª¤ 3: é‡åŒ–ä¸º {FINAL_GGUF_TYPE} æˆåŠŸ! ---")
        print(f"\nğŸ‰ æœ€ç»ˆæ¨¡å‹æ–‡ä»¶å·²ç”Ÿæˆåœ¨: {final_gguf_path}")
        print("æ‚¨ç°åœ¨å¯ä»¥å°†å…¶å¤åˆ¶åˆ° Ollama æœºå™¨ä¸Šå¹¶åˆ›å»º Modelfile äº†.")

    except subprocess.CalledProcessError as e:
        print(f"GGUF é‡åŒ–å¤±è´¥. é”™è¯¯ä¿¡æ¯: {e}")
    except FileNotFoundError:
        print(f"é”™è¯¯: æ— æ³•æ‰§è¡Œ {quantize_exe}. æƒé™æ˜¯å¦æ­£ç¡®?")

if __name__ == "__main__":
    main()